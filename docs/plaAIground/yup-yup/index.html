<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>boom - voice notes</title>
  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      padding: 20px;
      max-width: 600px;
      margin: 0 auto;
      background-color: #f7f9fc;
    }
    
    h1 {
      text-align: center;
      color: #333;
      margin-bottom: 25px;
    }
    
    .highlight {
      color: #1DB954;
      font-size: 1.4em;
      letter-spacing: -0.5px;
      display: block;
      margin-bottom: 5px;
      position: relative;
    }
    
    .highlight::before,
    .highlight::after {
      content: '"';
      font-size: 1.2em;
      color: #1DB954;
      opacity: 0.6;
    }
    
    .subtitle {
      font-size: 0.6em;
      color: #777;
      font-weight: normal;
      display: block;
    }
    
    /* Listening animation */
    @keyframes pulse {
      0% { transform: scale(1); opacity: 1; }
      50% { transform: scale(1.1); opacity: 0.8; }
      100% { transform: scale(1); opacity: 1; }
    }
    
    .listening-indicator {
      display: inline-block;
      width: 12px;
      height: 12px;
      border-radius: 50%;
      background-color: #1DB954;
      margin-right: 8px;
      display: none;
    }
    
    .listening-indicator.active {
      display: inline-block;
      animation: pulse 1.5s infinite ease-in-out;
    }
    
    .app-container {
      background: white;
      border-radius: 12px;
      padding: 20px;
      box-shadow: 0 4px 12px rgba(0,0,0,0.1);
    }
    
    .control-panel {
      display: flex;
      justify-content: center;
      margin-bottom: 20px;
    }
    
    button {
      background-color: #1DB954; /* Spotify green */
      color: white;
      border: none;
      padding: 15px 30px;
      margin: 0 10px;
      border-radius: 30px;
      cursor: pointer;
      font-weight: bold;
      font-size: 1rem;
      transition: all 0.2s;
    }
    
    button:hover {
      background-color: #1aa34a;
      transform: scale(1.05);
    }
    
    button:disabled {
      background-color: #ccc;
      cursor: not-allowed;
      transform: none;
    }
    
    button.record {
      background-color: #e74c3c;
    }
    
    button.record:hover {
      background-color: #c0392b;
    }
    
    .status {
      text-align: center;
      margin: 10px 0;
      font-weight: bold;
      height: 1.5em;
    }
    
    .transcript-area {
      width: 100%;
      min-height: 100px;
      margin: 20px 0;
      padding: 15px;
      border: 1px solid #ddd;
      border-radius: 8px;
      box-sizing: border-box;
      font-size: 1rem;
    }
    
    .note-list {
      margin-top: 30px;
    }
    
    .note-item {
      background-color: #f0f0f0;
      padding: 15px;
      margin-bottom: 10px;
      border-radius: 8px;
      position: relative;
    }
    
    .note-metadata {
      font-size: 0.8rem;
      color: #666;
      margin-top: 5px;
    }
    
    .note-controls {
      display: flex;
      justify-content: flex-end;
      gap: 10px;
      margin-top: 10px;
    }
    
    .note-controls button {
      padding: 5px 10px;
      font-size: 0.8rem;
    }
    
    .delete-btn {
      background-color: #e74c3c;
    }
    
    .spotify-container {
      margin: 20px 0;
      padding: 15px;
      background-color: #f0f0f0;
      border-radius: 8px;
      text-align: center;
    }
    
    .spotify-status {
      font-weight: bold;
    }
    
    .model-loading {
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      margin: 15px 0;
      padding: 10px;
      background-color: #f8f9fa;
      border-radius: 8px;
    }
    
    .progress-bar {
      width: 100%;
      height: 8px;
      background-color: #eee;
      border-radius: 4px;
      margin-top: 10px;
      overflow: hidden;
    }
    
    .progress-bar-fill {
      height: 100%;
      background-color: #1DB954;
      width: 0%;
      transition: width 0.3s ease;
    }
    
    /* Mobile optimizations */
    @media (max-width: 480px) {
      body {
        padding: 10px;
      }
      
      button {
        padding: 12px 20px;
        font-size: 0.9rem;
      }
    }
  </style>
</head>
<body>
  <div class="app-container">
    <h1><span class="highlight">boom</span> <span class="subtitle">voice notes</span></h1>
    
    <div id="spotify-connect" class="spotify-container">
      <p>Spotify integration is <span class="spotify-status">not connected</span></p>
      <button id="spotify-login-btn">Connect Spotify</button>
    </div>
    
    <div id="model-loading" class="model-loading">
      <p>Loading voice recognition model... <span id="model-status">0%</span></p>
      <div class="progress-bar">
        <div id="progress-bar-fill" class="progress-bar-fill"></div>
      </div>
    </div>
    
    <div class="voice-trigger-info" style="text-align: center; margin: 10px 0; padding: 15px; background-color: #f0f7ff; border-radius: 8px;">
      <div style="display: flex; justify-content: center; align-items: center; margin-bottom: 10px;">
        <div class="listening-indicator" id="listening-dot"></div>
        <p><strong>Voice Commands:</strong></p>
      </div>
      <div style="background-color: #e3f2fd; padding: 12px; border-radius: 6px; margin-bottom: 10px;">
        <p style="margin: 0; font-weight: bold;">Say <span style="color: #1DB954;">"boom"</span> once â†’ Start recording</p>
        <p style="margin: 5px 0 0 0; font-weight: bold;">Say <span style="color: #1DB954;">"boom"</span> again â†’ Stop recording</p>
      </div>
      <p style="color: #666; font-size: 0.9rem; margin-top: 10px;">
        <strong>Troubleshooting:</strong> Make sure you're online and have granted microphone permissions.
        Voice recognition works even offline once the model is loaded.
      </p>
      <p style="color: #2e7d32; font-size: 0.9rem; margin-top: 10px;">
        <strong>Spotify Feature:</strong> When you stop recording, your Spotify track will automatically rewind by 15 seconds.
        This helps you capture the context of what you were listening to.
      </p>
      <div id="browser-compatibility" style="margin-top: 12px; font-size: 0.85rem; color: #555;"></div>
    </div>
    
    <div class="control-panel">
      <button id="start-btn" disabled>Start Listening</button>
      <button id="record-btn" class="record" disabled>Record Note</button>
    </div>
    
    <div style="text-align: center; margin: 10px 0; padding: 8px; background-color: #f8f9fa; border-radius: 6px; display: flex; align-items: center; justify-content: center;">
      <div id="listening-status" style="display: inline-block; width: 12px; height: 12px; border-radius: 50%; background-color: #ccc; margin-right: 10px;"></div>
      <span id="listening-text">Not listening - press Start Listening</span>
    </div>
    
    <p id="status" class="status">Loading voice recognition...</p>
    
    <div id="debug-speech" style="font-size: 0.8rem; color: #666; background-color: #f5f5f5; padding: 8px; margin-bottom: 10px; border-radius: 4px; display: none;">No speech detected yet</div>
    
    <textarea id="transcript" class="transcript-area" placeholder="Your transcribed notes will appear here..." readonly></textarea>
    
    <div class="control-panel">
      <button id="save-btn" disabled>Save Note</button>
      <button id="clear-btn" disabled>Clear</button>
    </div>
    
    <div class="note-list" id="notes-container">
      <div style="display: flex; justify-content: space-between; align-items: center;">
        <h2>Saved Notes</h2>
        <button id="export-notes-btn" style="margin-left: 10px;">Export Notes</button>
      </div>
      <!-- Notes will be added here dynamically -->
    </div>
  </div>

  <!-- Speech Recognition Libraries -->
  <script src="https://cdn.jsdelivr.net/npm/vosk-browser@0.0.8/dist/vosk.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/vosk@0.3.47/vosk.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@vosk/browser@0.0.8/dist/vosk.js"></script>
  
  <!-- Fallback to Web Speech API if Vosk doesn't work -->
  <script>
    console.log("Checking if Vosk loaded correctly");
    
    // Simple check if Vosk is available - will set a flag to use Web Speech API if not
    window.voskAvailable = false;
    
    try {
      if (typeof Vosk === 'function' || typeof vosk !== 'undefined' || typeof window.vosk !== 'undefined') {
        console.log("Vosk appears to be available");
        window.voskAvailable = true;
      } else {
        console.warn("Vosk doesn't appear to be available");
        window.voskAvailable = false;
      }
    } catch(e) {
      console.error("Error checking Vosk:", e);
      window.voskAvailable = false;
    }
  </script>

  <script>
    // DOM Elements
    const startBtn = document.getElementById('start-btn');
    const recordBtn = document.getElementById('record-btn');
    const saveBtn = document.getElementById('save-btn');
    const clearBtn = document.getElementById('clear-btn');
    const spotifyLoginBtn = document.getElementById('spotify-login-btn');
    const statusDisplay = document.getElementById('status');
    const transcriptArea = document.getElementById('transcript');
    const notesContainer = document.getElementById('notes-container');
    const modelStatus = document.getElementById('model-status');
    const progressBarFill = document.getElementById('progress-bar-fill');
    const modelLoadingContainer = document.getElementById('model-loading');
    
    // Debug mode - set to true to see more detailed logs
    const DEBUG_MODE = true;
    const debugArea = document.getElementById('debug-speech');
    if (debugArea && DEBUG_MODE) {
      debugArea.style.display = 'block';
    }
    
    // App state
    let isListening = false;
    let isRecording = false;
    let spotifyConnected = false;
    let spotifyToken = null;
    let currentSpotifyTrack = null;
    let currentPlaybackPosition = 0; // Store the current playback position
    const REWIND_SECONDS = 15; // How many seconds to rewind when stopping recording
    
    // Voice trigger settings
    const TRIGGER_PHRASE = "boom";
    let lastTriggerTime = 0;
    const TRIGGER_COOLDOWN_MS = 2000; // Cooldown between triggers (2 seconds)
    
    // Vosk variables
    let vosk;
    let model;
    let recognizer;
    let audioContext;
    let mediaStream;
    let scriptProcessor;
    let modelReady = false;
    
    // Get listening indicator element
    const listeningDot = document.getElementById('listening-dot');
    
    // Load saved notes from localStorage
    loadNotes();
    
    // Initialize Vosk model or fall back to Web Speech API
    initVosk();
    
    // Check if returning from Spotify auth
    checkSpotifyAuth();
    
    // Web Speech API fallback
    function setupWebSpeechAPI() {
      console.log('Setting up Web Speech API fallback');
      modelLoadingContainer.style.display = 'none';
      statusDisplay.textContent = 'Using browser speech recognition';
      
      // Hide Vosk-specific elements
      const voskElements = document.querySelectorAll('.vosk-only');
      voskElements.forEach(el => el.style.display = 'none');
      
      // Show Web Speech API specific elements
      const webSpeechElements = document.querySelectorAll('.webspeech-only');
      webSpeechElements.forEach(el => el.style.display = 'block');
      
      // Initialize Web Speech API
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      if (!SpeechRecognition) {
        statusDisplay.textContent = 'Speech recognition not supported in this browser';
        startBtn.disabled = true;
        return;
      }
      
      // Create recognition instance
      recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'en-US';
      recognition.maxAlternatives = 5;
      
      // Set up event handlers
      setupWebSpeechEvents();
      
      // Enable start button
      startBtn.disabled = false;
      
      // Mark as ready using the same flag as Vosk for compatibility
      modelReady = true;
    }
    
    function setupWebSpeechEvents() {
      if (!recognition) return;
      
      recognition.onresult = handleWebSpeechResult;
      recognition.onerror = handleWebSpeechError;
      recognition.onend = handleWebSpeechEnd;
    }
    
    function handleWebSpeechResult(event) {
      let interimTranscript = '';
      let finalTranscript = '';
      
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript.toLowerCase();
        const confidence = event.results[i][0].confidence;
        
        if (DEBUG_MODE) {
          console.log(`Speech detected: "${transcript}" (confidence: ${confidence.toFixed(2)})`);
          if (debugArea) {
            debugArea.textContent = `Last detected: "${transcript}" (confidence: ${confidence.toFixed(2)})`;
          }
        }
        
        // Process results similarly to how we would with Vosk
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
          
          // If recording, update transcript
          if (isRecording) {
            // Remove trigger phrase from beginning or end of transcript
            const triggerPattern = new RegExp(`^${TRIGGER_PHRASE}\\s*|\\s*${TRIGGER_PHRASE}$`, 'gi');
            let cleanTranscript = transcript.replace(triggerPattern, '').trim();
            
            // Only add non-empty transcripts
            if (cleanTranscript) {
              // Append to existing transcript with a space
              if (transcriptArea.value) {
                transcriptArea.value += ' ' + cleanTranscript;
              } else {
                transcriptArea.value = cleanTranscript;
              }
              
              // Enable save and clear buttons if we have text
              if (transcriptArea.value.trim()) {
                saveBtn.disabled = false;
                clearBtn.disabled = false;
              }
            }
          }
          
          // Check for trigger phrase (using original transcript)
          checkForTriggerPhrase(transcript);
        } else {
          interimTranscript += transcript;
          
          // If recording, show interim results but preserve previous final text
          if (isRecording) {
            // For interim results, we show current chunk but don't append yet
            // Remove trigger phrase for display purposes
            const triggerPattern = new RegExp(`^${TRIGGER_PHRASE}\\s*|\\s*${TRIGGER_PHRASE}$`, 'gi');
            let cleanInterim = interimTranscript.replace(triggerPattern, '').trim();
            transcriptArea.value = cleanInterim;
          }
          
          // Check for trigger in interim results for faster response
          checkForTriggerPhrase(interimTranscript);
        }
      }
    }
    
    function handleWebSpeechError(event) {
      console.error('Speech recognition error:', event.error);
      
      if (event.error === 'no-speech') {
        // Just restart if no speech detected
        if (isListening) {
          try {
            recognition.start();
          } catch (e) {
            console.error('Could not restart recognition after no-speech', e);
          }
        }
      } else {
        statusDisplay.textContent = `Error: ${event.error}. Try refreshing.`;
      }
    }
    
    function handleWebSpeechEnd() {
      if (isListening) {
        // Restart if we're supposed to be listening
        try {
          recognition.start();
        } catch (e) {
          console.error('Error restarting recognition in onend handler:', e);
        }
      }
    }
    
    // Event listeners
    startBtn.addEventListener('click', toggleListening);
    recordBtn.addEventListener('click', toggleRecording);
    saveBtn.addEventListener('click', saveNote);
    clearBtn.addEventListener('click', clearTranscript);
    spotifyLoginBtn.addEventListener('click', connectToSpotify);
    document.getElementById('export-notes-btn').addEventListener('click', exportNotes);
    
    // Initialize Vosk speech recognition
    async function initVosk() {
      try {
        // Try different approaches to access Vosk
        if (typeof Vosk === 'function') {
          console.log('Using Vosk constructor function');
          vosk = await Vosk();
        } else if (typeof window.Vosk === 'function') {
          console.log('Using window.Vosk constructor function');
          vosk = await window.Vosk();
        } else if (typeof vosk !== 'undefined') {
          console.log('Using predefined vosk object');
          // vosk is already defined - no need to do anything
        } else if (typeof window.vosk !== 'undefined') {
          console.log('Using window.vosk object');
          vosk = window.vosk;
        } else {
          console.error('No Vosk implementation found - using Web Speech API instead');
          // Fall back to Web Speech API
          setupWebSpeechAPI();
          return;
        }
        
        console.log('Vosk object:', vosk);
        
        // For testing only - if we can't properly use Vosk, use a fallback
        if (!vosk || typeof vosk.setProgressCallback !== 'function' || typeof vosk.createModel !== 'function') {
          console.warn('Invalid Vosk implementation - using Web Speech API instead');
          setupWebSpeechAPI();
          return;
        }
        
        // Model download progress event
        vosk.setProgressCallback((progress) => {
          const percent = Math.floor(progress * 100);
          modelStatus.textContent = `${percent}%`;
          progressBarFill.style.width = `${percent}%`;
          
          if (DEBUG_MODE) {
            console.log(`Model download progress: ${percent}%`);
          }
        });
        
        // Load the small English model
        statusDisplay.textContent = 'Downloading speech recognition model...';
        model = await vosk.createModel('https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip');
        
        // Hide the loading display
        modelLoadingContainer.style.display = 'none';
        
        // Create recognizer instance
        recognizer = new vosk.Recognizer({model: model, sampleRate: 16000});
        recognizer.setMaxAlternatives(5);
        recognizer.setWords(true);
        
        statusDisplay.textContent = 'Voice recognition ready! Press "Start Listening"';
        startBtn.disabled = false;
        modelReady = true;
        
        console.log('Vosk model loaded successfully');
      } catch (error) {
        console.error('Error initializing Vosk:', error);
        statusDisplay.textContent = 'Error loading voice recognition model. Please refresh and try again.';
        modelStatus.textContent = 'Error';
        progressBarFill.style.backgroundColor = '#e74c3c';
      }
    }
    
    // Functions
    async function toggleListening() {
      if (!isListening) {
        if (!modelReady) {
          statusDisplay.textContent = 'Speech recognition model not loaded yet. Please wait.';
          return;
        }
        
        try {
          // Request microphone permission
          mediaStream = await navigator.mediaDevices.getUserMedia({audio: true});
          
          // Start recognition based on which API we're using
          if (recognition) {
            // We're using Web Speech API
            try {
              recognition.start();
            } catch (e) {
              console.error('Error starting Web Speech recognition:', e);
              statusDisplay.textContent = 'Error starting speech recognition.';
              return;
            }
          } else if (vosk && recognizer) {
            // We're using Vosk
            startAudioProcessing();
          } else {
            console.error('No speech recognition engine available');
            statusDisplay.textContent = 'Speech recognition not available.';
            return;
          }
          
          isListening = true;
          startBtn.textContent = 'Stop Listening';
          statusDisplay.textContent = 'Listening for "boom"...';
          recordBtn.disabled = false;
          
          // Update listening status indicators
          const listeningStatus = document.getElementById('listening-status');
          const listeningText = document.getElementById('listening-text');
          
          if (listeningStatus) {
            listeningStatus.style.backgroundColor = '#1DB954';
            listeningStatus.style.animation = 'pulse 1.5s infinite ease-in-out';
          }
          
          if (listeningText) {
            listeningText.textContent = 'Listening - say "boom" to start recording';
          }
          
          // Activate listening indicator
          listeningDot.classList.add('active');
          
        } catch (error) {
          console.error('Error accessing microphone:', error);
          statusDisplay.textContent = 'Error: Could not access microphone. Please allow microphone access.';
        }
      } else {
        // Stop listening based on which API we're using
        if (recognition) {
          // We're using Web Speech API
          try {
            recognition.stop();
          } catch (e) {
            console.error('Error stopping Web Speech recognition:', e);
          }
        } else if (vosk && recognizer) {
          // We're using Vosk
          stopAudioProcessing();
        }
        
        isListening = false;
        startBtn.textContent = 'Start Listening';
        statusDisplay.textContent = 'Not listening';
        recordBtn.disabled = true;
        
        if (isRecording) {
          toggleRecording();
        }
        
        // Update listening status indicators
        const listeningStatus = document.getElementById('listening-status');
        const listeningText = document.getElementById('listening-text');
        
        if (listeningStatus) {
          listeningStatus.style.backgroundColor = '#ccc';
          listeningStatus.style.animation = 'none';
        }
        
        if (listeningText) {
          listeningText.textContent = 'Not listening - press Start Listening';
        }
        
        // Deactivate listening indicator
        listeningDot.classList.remove('active');
      }
    }
    
    function startAudioProcessing() {
      // Create audio context
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      
      // Create audio processor
      scriptProcessor = audioContext.createScriptProcessor(4096, 1, 1);
      
      // Create source node
      const source = audioContext.createMediaStreamSource(mediaStream);
      
      // Connect nodes
      source.connect(scriptProcessor);
      scriptProcessor.connect(audioContext.destination);
      
      // Process audio
      scriptProcessor.addEventListener('audioprocess', processAudio);
    }
    
    function stopAudioProcessing() {
      if (scriptProcessor) {
        scriptProcessor.removeEventListener('audioprocess', processAudio);
        scriptProcessor.disconnect();
        scriptProcessor = null;
      }
      
      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }
      
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
        mediaStream = null;
      }
      
      if (recognizer) {
        recognizer.reset();
      }
    }
    
    function processAudio(audioProcessingEvent) {
      if (!recognizer || !isListening) return;
      
      // Get audio samples
      const inputData = audioProcessingEvent.inputBuffer.getChannelData(0);
      
      // Process through Vosk
      const sampleFloat32 = convertFloat32ToInt16(inputData);
      const result = recognizer.acceptWaveform(sampleFloat32);
      
      if (result) {
        // We have a final result
        const finalResult = recognizer.result();
        
        if (DEBUG_MODE) {
          console.log('Vosk result:', finalResult);
          if (debugArea) {
            debugArea.textContent = `Last detected: "${finalResult.text}"`;
          }
        }
        
        processSpeechResult(finalResult);
      } else {
        // We have a partial result
        const partialResult = recognizer.partialResult();
        
        if (DEBUG_MODE) {
          console.log('Vosk partial result:', partialResult);
        }
        
        if (isRecording) {
          // For partial results, preserve the existing text but show what's being processed
          // Remove trigger phrase
          const triggerPattern = new RegExp(`^${TRIGGER_PHRASE}\\s*|\\s*${TRIGGER_PHRASE}$`, 'gi');
          let cleanPartial = partialResult.partial.replace(triggerPattern, '').trim();
          
          // Get the current completed text (we'll add our partial after it)
          const currentText = transcriptArea.value || '';
          const partialIndicator = ' â—'; // Show a dot to indicate interim text
          
          // Only update if we have a meaningful partial result
          if (cleanPartial) {
            // Find if there's already a partial indicator and remove it
            const existingPartialIndex = currentText.lastIndexOf(partialIndicator);
            const baseText = existingPartialIndex !== -1 
              ? currentText.substring(0, existingPartialIndex)
              : currentText;
              
            // Set the text with the new partial
            transcriptArea.value = baseText + (baseText && cleanPartial ? ' ' : '') + cleanPartial + partialIndicator;
          }
        }
        
        // Check for trigger word in partial results for better responsiveness
        checkForTriggerPhrase(partialResult.partial);
      }
    }
    
    function processSpeechResult(result) {
      if (!result || !result.text) return;
      
      let transcript = result.text.toLowerCase();
      
      // If we're recording, update the transcript
      if (isRecording) {
        // Remove trigger phrase from beginning or end of transcript
        const triggerPattern = new RegExp(`^${TRIGGER_PHRASE}\s*|\s*${TRIGGER_PHRASE}$`, 'gi');
        transcript = transcript.replace(triggerPattern, '').trim();
        
        // Only add non-empty transcripts
        if (transcript) {
          // Append to existing transcript with a space
          if (transcriptArea.value) {
            transcriptArea.value += ' ' + transcript;
          } else {
            transcriptArea.value = transcript;
          }
          
          // Enable save and clear buttons if we have text
          if (transcriptArea.value.trim()) {
            saveBtn.disabled = false;
            clearBtn.disabled = false;
          }
        }
      }
      
      // Check for trigger phrase (using original transcript)
      checkForTriggerPhrase(result.text.toLowerCase());
    }
    
    function checkForTriggerPhrase(transcript) {
      if (!transcript) return;
      
      // Convert to lowercase for comparison
      const lowerTranscript = transcript.toLowerCase();
      
      // Check if transcript contains our trigger phrase
      if (lowerTranscript.includes(TRIGGER_PHRASE)) {
        console.log(`Possible trigger: "${lowerTranscript}" contains "${TRIGGER_PHRASE}"`);
        
        // Check if we're outside the cooldown period
        const currentTime = Date.now();
        if (currentTime - lastTriggerTime > TRIGGER_COOLDOWN_MS) {
          console.log('ðŸŽ¯ TRIGGER PHRASE DETECTED!', lowerTranscript);
          statusDisplay.textContent = 'Boom detected!';
          
          // Play audio feedback
          playTriggerSound();
          
          // Toggle recording state
          toggleRecording();
          
          // Update last trigger time
          lastTriggerTime = currentTime;
          
          // Flash the visual indicator
          listeningDot.style.backgroundColor = '#e74c3c';
          setTimeout(() => {
            listeningDot.style.backgroundColor = '#1DB954';
          }, 500);
          
          // Show big visual confirmation of trigger
          showTriggerConfirmation();
        } else {
          console.log('Trigger cooldown active - ignoring trigger');
        }
      }
      
      // Also check for similar phrases that might be misrecognized
      const similarPhrases = ["boom boom", "boo", "boon", "boomb", "bum"];
      for (const phrase of similarPhrases) {
        if (lowerTranscript.includes(phrase)) {
          console.log(`SIMILAR PHRASE DETECTED: "${phrase}" in "${lowerTranscript}"`);
          
          // Check if we're outside the cooldown period
          const currentTime = Date.now();
          if (currentTime - lastTriggerTime > TRIGGER_COOLDOWN_MS) {
            console.log('ðŸŽ¯ SIMILAR TRIGGER PHRASE DETECTED!', lowerTranscript);
            statusDisplay.textContent = `"${phrase}" detected (similar to "boom")!`;
            
            // Play audio feedback
            playTriggerSound();
            
            // Toggle recording state
            toggleRecording();
            
            // Update last trigger time
            lastTriggerTime = currentTime;
            
            // Flash the visual indicator
            listeningDot.style.backgroundColor = '#e74c3c';
            setTimeout(() => {
              listeningDot.style.backgroundColor = '#1DB954';
            }, 500);
            
            // Show big visual confirmation of trigger
            showTriggerConfirmation();
          }
          break;
        }
      }
    }
    
    function playTriggerSound() {
      const feedback = new Audio();
      feedback.src = 'data:audio/mp3;base64,SUQzBAAAAAAAI1RTU0UAAAAPAAADTGF2ZjU4Ljc2LjEwMAAAAAAAAAAAAAAA//tAwAAAAAAAAAAAAAAAAAAAAAAAWGluZwAAAA8AAAACAAADmgD///////////////////////////////////////////8AAAA8TEFNRTMuMTAwAc0AAAAAAAAAABSAJALFQQAAgAAAA5rKPlWnAAAAAAAAAAAAAAAAAAAA//tQwAADB0QLXBGMAKIIFmRDKMAAJAEcOHDhw4HAcEAQDgAoJh9/+CYP/BMP/y4Pggf/BBMMEwTD////5MPggH/5cHz/wTBMEwf////BMEAQDhw4cOHDgQBAEAQDh82s9LW+XFx0yTgEAQBAEA//+AQbDgEAQOCAgcmRH9//LlxHF/y4/+XIAgCDhw4cOHDhw4cOHDhw4cOHDhw4cOHDhw4cOHDhw4cOHDhw4cOHDhw4cOHDhw4cOHDhw4cOHDhw4cOHDhw4cOHD';
      feedback.play();
    }
    
    function showTriggerConfirmation() {
      const triggerConfirm = document.createElement('div');
      triggerConfirm.textContent = 'BOOM!';
      triggerConfirm.style.position = 'fixed';
      triggerConfirm.style.top = '50%';
      triggerConfirm.style.left = '50%';
      triggerConfirm.style.transform = 'translate(-50%, -50%)';
      triggerConfirm.style.backgroundColor = '#1DB954';
      triggerConfirm.style.color = 'white';
      triggerConfirm.style.fontSize = '32px';
      triggerConfirm.style.padding = '20px 40px';
      triggerConfirm.style.borderRadius = '10px';
      triggerConfirm.style.zIndex = '1000';
      triggerConfirm.style.fontWeight = 'bold';
      triggerConfirm.style.boxShadow = '0 4px 10px rgba(0,0,0,0.2)';
      document.body.appendChild(triggerConfirm);
      
      setTimeout(() => {
        document.body.removeChild(triggerConfirm);
      }, 1000);
    }
    
    function toggleRecording() {
      // Get status indicators
      const listeningStatus = document.getElementById('listening-status');
      const listeningText = document.getElementById('listening-text');
      
      if (!isRecording) {
        // Start recording
        isRecording = true;
        recordBtn.textContent = 'Stop Recording';
        statusDisplay.textContent = 'Recording note...';
        transcriptArea.value = '';
        saveBtn.disabled = true;
        clearBtn.disabled = true;
        
        // Play audio feedback
        playAudioFeedback('start');
        
        // If Spotify is connected, pause it
        if (spotifyConnected) {
          pauseSpotify();
        }
        
        // Visual feedback - change button color and add pulsing dot
        recordBtn.style.backgroundColor = '#e74c3c';
        document.querySelector('.listening-indicator').style.backgroundColor = '#e74c3c';
        
        // Update status indicators
        if (listeningStatus) {
          listeningStatus.style.backgroundColor = '#e74c3c';
          listeningStatus.style.animation = 'pulse 1s infinite ease-in-out';
        }
        
        if (listeningText) {
          listeningText.textContent = 'RECORDING - say "boom" again to stop';
          listeningText.style.fontWeight = 'bold';
          listeningText.style.color = '#e74c3c';
        }
      } else {
        // Stop recording
        isRecording = false;
        recordBtn.textContent = 'Record Note';
        statusDisplay.textContent = 'Note recorded and saved';
        saveBtn.disabled = false;
        clearBtn.disabled = false;
        
        // Play audio feedback
        playAudioFeedback('stop');
        
        // Automatically save the note if there's content
        if (transcriptArea.value.trim()) {
          saveNote();
        }
        
        // If Spotify is connected, resume it
        if (spotifyConnected) {
          resumeSpotify();
        }
        
        // Visual feedback - restore button color
        recordBtn.style.backgroundColor = '#1DB954';
        document.querySelector('.listening-indicator').style.backgroundColor = '#1DB954';
        
        // Update status indicators
        if (listeningStatus) {
          listeningStatus.style.backgroundColor = '#1DB954';
          listeningStatus.style.animation = 'pulse 1.5s infinite ease-in-out';
        }
        
        if (listeningText) {
          listeningText.textContent = 'Listening - say "boom" to start recording';
          listeningText.style.fontWeight = 'normal';
          listeningText.style.color = 'initial';
        }
      }
      
      // Console log for debugging
      console.log('Recording state toggled to:', isRecording ? 'RECORDING' : 'NOT RECORDING');
    }
    
    // Audio feedback function
    function playAudioFeedback(type) {
      const audioContext = new (window.AudioContext || window.webkitAudioContext)();
      const oscillator = audioContext.createOscillator();
      const gainNode = audioContext.createGain();
      
      oscillator.connect(gainNode);
      gainNode.connect(audioContext.destination);
      
      if (type === 'start') {
        // Higher pitch for start
        oscillator.frequency.value = 880; // A5
        gainNode.gain.value = 0.2;
        oscillator.start();
        setTimeout(() => {
          oscillator.stop();
        }, 200);
      } else if (type === 'stop') {
        // Lower pitch for stop
        oscillator.frequency.value = 440; // A4
        gainNode.gain.value = 0.2;
        oscillator.start();
        setTimeout(() => {
          oscillator.stop();
        }, 200);
      }
    }
    
    function saveNote() {
      if (transcriptArea.value.trim() === '') {
        statusDisplay.textContent = 'Cannot save empty note';
        return;
      }
      
      const note = {
        id: Date.now(),
        text: transcriptArea.value.trim(),
        timestamp: new Date().toISOString(),
        source: spotifyConnected && currentSpotifyTrack ? 
          `Spotify - ${currentSpotifyTrack.name} by ${currentSpotifyTrack.artists.map(a => a.name).join(', ')}` :
          'Manual note'
      };
      
      // Save to localStorage
      const notes = JSON.parse(localStorage.getItem('secondBrainNotes') || '[]');
      notes.push(note);
      localStorage.setItem('secondBrainNotes', JSON.stringify(notes));
      
      // Add to UI
      addNoteToUI(note);
      
      // Clear transcript
      transcriptArea.value = '';
      statusDisplay.textContent = 'Note saved';
      saveBtn.disabled = true;
      clearBtn.disabled = true;
    }
    
    function clearTranscript() {
      transcriptArea.value = '';
      statusDisplay.textContent = 'Transcript cleared';
      saveBtn.disabled = true;
      clearBtn.disabled = true;
    }
    
    function loadNotes() {
      const notes = JSON.parse(localStorage.getItem('secondBrainNotes') || '[]');
      
      // Clear container first
      const notesHeader = document.createElement('h2');
      notesHeader.textContent = 'Saved Notes';
      notesContainer.innerHTML = '';
      notesContainer.appendChild(notesHeader);
      
      // Add notes to UI
      notes.forEach(note => addNoteToUI(note));
    }
    
    function addNoteToUI(note) {
      const noteElement = document.createElement('div');
      noteElement.className = 'note-item';
      noteElement.dataset.id = note.id;
      
      const dateOptions = { year: 'numeric', month: 'short', day: 'numeric', hour: '2-digit', minute: '2-digit' };
      const formattedDate = new Date(note.timestamp).toLocaleDateString('en-US', dateOptions);
      
      noteElement.innerHTML = `
        <div class="note-text">${note.text}</div>
        <div class="note-metadata">
          <div>Source: ${note.source}</div>
          <div>Created: ${formattedDate}</div>
        </div>
        <div class="note-controls">
          <button class="edit-btn">Edit</button>
          <button class="delete-btn">Delete</button>
        </div>
      `;
      
      // Add event listeners for edit and delete
      noteElement.querySelector('.delete-btn').addEventListener('click', () => deleteNote(note.id));
      noteElement.querySelector('.edit-btn').addEventListener('click', () => editNote(note.id));
      
      // Add to container
      notesContainer.appendChild(noteElement);
    }
    
    function deleteNote(id) {
      // Remove from localStorage
      let notes = JSON.parse(localStorage.getItem('secondBrainNotes') || '[]');
      notes = notes.filter(note => note.id !== id);
      localStorage.setItem('secondBrainNotes', JSON.stringify(notes));
      
      // Remove from UI
      const noteElement = document.querySelector(`.note-item[data-id="${id}"]`);
      if (noteElement) {
        noteElement.remove();
      }
      
      statusDisplay.textContent = 'Note deleted';
    }
    
    function editNote(id) {
      // Get note data
      const notes = JSON.parse(localStorage.getItem('secondBrainNotes') || '[]');
      const note = notes.find(note => note.id === id);
      
      if (note) {
        // Load into textarea
        transcriptArea.value = note.text;
        saveBtn.disabled = false;
        clearBtn.disabled = false;
        
        // Remove old note
        deleteNote(id);
        
        statusDisplay.textContent = 'Editing note';
      }
    }
    
    // Helper function to convert audio data for Vosk
    function convertFloat32ToInt16(buffer) {
      const l = buffer.length;
      const buf = new Int16Array(l);
      
      for (let i = 0; i < l; i++) {
        buf[i] = Math.min(1, Math.max(-1, buffer[i])) * 0x7FFF;
      }
      
      return buf.buffer;
    }
    
    // Spotify Integration
    function connectToSpotify() {
      // Spotify API credentials with your client ID
      const client_id = '39a9e6d4d9f24dcd8dcbecd89118649b';
      const redirect_uri = window.location.origin + window.location.pathname;
      const scope = 'user-read-playback-state user-modify-playback-state user-read-currently-playing';
      
      // Save redirect state
      localStorage.setItem('spotifyAuthPending', 'true');
      
      // Redirect to Spotify auth
      window.location.href = `https://accounts.spotify.com/authorize?client_id=${client_id}&response_type=token&redirect_uri=${encodeURIComponent(redirect_uri)}&scope=${encodeURIComponent(scope)}`;
    }
    
    function checkSpotifyAuth() {
      // Check if we're returning from Spotify auth
      const hashParams = getHashParams();
      if (hashParams.access_token) {
        spotifyToken = hashParams.access_token;
        spotifyConnected = true;
        localStorage.setItem('spotifyToken', spotifyToken);
        localStorage.setItem('spotifyTokenExpiry', Date.now() + (hashParams.expires_in * 1000));
        
        // Clear hash from URL
        window.history.replaceState(null, null, ' ');
        
        updateSpotifyUI();
        getCurrentTrack();
      } else {
        // Check if we have a valid token in localStorage
        const savedToken = localStorage.getItem('spotifyToken');
        const tokenExpiry = localStorage.getItem('spotifyTokenExpiry');
        
        if (savedToken && tokenExpiry && Date.now() < parseInt(tokenExpiry)) {
          spotifyToken = savedToken;
          spotifyConnected = true;
          updateSpotifyUI();
          getCurrentTrack();
        }
      }
    }
    
    function getHashParams() {
      const hashParams = {};
      let e, r = /([^&;=]+)=?([^&;]*)/g,
          q = window.location.hash.substring(1);
          
      while (e = r.exec(q)) {
        hashParams[e[1]] = decodeURIComponent(e[2]);
      }
      
      return hashParams;
    }
    
    function updateSpotifyUI() {
      document.querySelector('.spotify-status').textContent = spotifyConnected ? 'connected' : 'not connected';
      spotifyLoginBtn.textContent = spotifyConnected ? 'Refresh Connection' : 'Connect Spotify';
      
      if (spotifyConnected) {
        const spotifyContainer = document.getElementById('spotify-connect');
        
        // Check if track info element already exists
        let trackInfoElement = document.getElementById('current-track-info');
        if (!trackInfoElement) {
          trackInfoElement = document.createElement('div');
          trackInfoElement.id = 'current-track-info';
          trackInfoElement.style.marginTop = '10px';
          spotifyContainer.appendChild(trackInfoElement);
        }
      }
    }
    
    function getCurrentTrack() {
      if (!spotifyConnected || !spotifyToken) return;
      
      fetch('https://api.spotify.com/v1/me/player/currently-playing', {
        headers: {
          'Authorization': `Bearer ${spotifyToken}`
        }
      })
      .then(response => {
        if (response.status === 204) {
          // No track currently playing
          return null;
        }
        return response.json();
      })
      .then(data => {
        if (data && data.item) {
          currentSpotifyTrack = data.item;
          // Store current playback position (in milliseconds)
          currentPlaybackPosition = data.progress_ms;
          
          const trackInfoElement = document.getElementById('current-track-info');
          if (trackInfoElement) {
            const formattedTime = formatTime(Math.floor(data.progress_ms / 1000));
            trackInfoElement.innerHTML = `
              <p>Now playing: <strong>${data.item.name}</strong> by ${data.item.artists.map(a => a.name).join(', ')}</p>
              <p>Current position: ${formattedTime}</p>
            `;
          }
        }
      })
      .catch(error => {
        console.error('Error getting current track', error);
        if (error.status === 401) {
          // Token expired
          spotifyConnected = false;
          updateSpotifyUI();
        }
      });
    }
    
    function pauseSpotify() {
      if (!spotifyConnected || !spotifyToken) return;
      
      fetch('https://api.spotify.com/v1/me/player/pause', {
        method: 'PUT',
        headers: {
          'Authorization': `Bearer ${spotifyToken}`
        }
      })
      .then(() => {
        console.log('Spotify paused');
      })
      .catch(error => {
        console.error('Error pausing Spotify', error);
      });
    }
    
    function resumeSpotify() {
      if (!spotifyConnected || !spotifyToken) return;
      
      // Calculate new position by rewinding the stored position by REWIND_SECONDS
      let rewindPositionMs = currentPlaybackPosition - (REWIND_SECONDS * 1000);
      
      // Make sure we don't go below zero
      rewindPositionMs = Math.max(0, rewindPositionMs);
      
      // Format time for logging
      const formattedOriginalTime = formatTime(Math.floor(currentPlaybackPosition / 1000));
      const formattedRewindTime = formatTime(Math.floor(rewindPositionMs / 1000));
      
      console.log(`Rewinding from ${formattedOriginalTime} to ${formattedRewindTime}`);
      
      // First resume playback
      fetch('https://api.spotify.com/v1/me/player/play', {
        method: 'PUT',
        headers: {
          'Authorization': `Bearer ${spotifyToken}`
        }
      })
      .then(() => {
        console.log('Spotify resumed, now seeking to rewound position');
        
        // Then seek to the rewound position
        return fetch(`https://api.spotify.com/v1/me/player/seek?position_ms=${rewindPositionMs}`, {
          method: 'PUT',
          headers: {
            'Authorization': `Bearer ${spotifyToken}`
          }
        });
      })
      .then(() => {
        console.log(`Spotify rewound ${REWIND_SECONDS} seconds to ${formattedRewindTime}`);
        
        // Update UI to show user what happened
        statusDisplay.textContent = `Spotify rewound ${REWIND_SECONDS} seconds`;
        
        // Update current track info after a moment to reflect new position
        setTimeout(getCurrentTrack, 1000);
      })
      .catch(error => {
        console.error('Error with Spotify resume/rewind:', error);
      });
    }
    
    // Helper function to format seconds into MM:SS format
    function formatTime(totalSeconds) {
      const minutes = Math.floor(totalSeconds / 60);
      const seconds = totalSeconds % 60;
      return `${minutes}:${seconds.toString().padStart(2, '0')}`;
    }
    
    // Export notes function
    function exportNotes() {
      const notes = JSON.parse(localStorage.getItem('secondBrainNotes') || '[]');
      if (notes.length === 0) {
        statusDisplay.textContent = 'No notes to export';
        return;
      }
      
      // Format notes for export
      let exportData = "# Exported Voice Notes\n\n";
      
      notes.forEach((note, index) => {
        const date = new Date(note.timestamp);
        const formattedDate = date.toLocaleDateString('en-US', { 
          year: 'numeric', 
          month: 'long', 
          day: 'numeric',
          hour: '2-digit',
          minute: '2-digit'
        });
        
        exportData += `## Note ${index + 1} - ${formattedDate}\n\n`;
        exportData += `${note.text}\n\n`;
        exportData += `Source: ${note.source}\n`;
        exportData += `Timestamp: ${note.timestamp}\n\n`;
        exportData += "---\n\n";
      });
      
      // Create a downloadable file
      const blob = new Blob([exportData], { type: 'text/markdown' });
      const url = URL.createObjectURL(blob);
      
      // Create a temporary link to download the file
      const a = document.createElement('a');
      a.href = url;
      a.download = `voice-notes-export-${new Date().toISOString().slice(0, 10)}.md`;
      document.body.appendChild(a);
      a.click();
      
      // Clean up
      setTimeout(() => {
        document.body.removeChild(a);
        URL.revokeObjectURL(url);
      }, 100);
      
      statusDisplay.textContent = `${notes.length} notes exported`;
    }
    
    // Poll for current track every 5 seconds if connected
    setInterval(() => {
      if (spotifyConnected) {
        getCurrentTrack();
      }
    }, 5000);
  </script>
</body>
</html>